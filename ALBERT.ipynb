{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Проверяем доступность GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных из CSV-файла\n",
    "df = pd.read_csv('your_dataset.csv')  # Замените 'your_dataset.csv' на путь к вашему файлу CSV\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Подготовка текстов и меток\n",
    "train_texts, train_labels = train_df['news_headline'].tolist(), train_df['category_num'].tolist()\n",
    "test_texts, test_labels = test_df['news_headline'].tolist(), test_df['category_num'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Инициализация токенизатора и преобразование текста в токены\n",
    "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Преобразование меток в тензоры\n",
    "train_labels = torch.tensor(train_labels).clone().detach()\n",
    "test_labels = torch.tensor(test_labels).clone().detach()\n",
    "\n",
    "# Создание DataLoader для обучения и тестирования\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "# Используйте DataLoader для автоматического перемещения данных на GPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Инициализация модели RoBERTa для классификации на GPU\n",
    "num_classes = 2  # Замените 2 на количество классов в вашем наборе данных\n",
    "\n",
    "# Загрузка оригинальной модели RoBERTa\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained('albert-base-v2')\n",
    "\n",
    "# Создание новой (менее объемной) модели с использованием distill\n",
    "albert_model = AutoModelForSequenceClassification.from_pretrained('albert-base-v2')\n",
    "albert_model.to(device)\n",
    "\n",
    "# Инициализация оптимизатора\n",
    "optimizer = torch.optim.AdamW(albert_model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/2094, Loss: 0.7553, Accuracy: 0.5250\n",
      "Epoch 1, Batch 20/2094, Loss: 0.7159, Accuracy: 0.5250\n",
      "Epoch 1, Batch 30/2094, Loss: 0.6586, Accuracy: 0.5917\n",
      "Epoch 1, Batch 40/2094, Loss: 0.6120, Accuracy: 0.6438\n",
      "Epoch 1, Batch 50/2094, Loss: 0.5555, Accuracy: 0.7000\n",
      "Epoch 1, Batch 60/2094, Loss: 0.5045, Accuracy: 0.7375\n",
      "Epoch 1, Batch 70/2094, Loss: 0.4689, Accuracy: 0.7643\n",
      "Epoch 1, Batch 80/2094, Loss: 0.4360, Accuracy: 0.7875\n",
      "Epoch 1, Batch 90/2094, Loss: 0.4199, Accuracy: 0.8028\n",
      "Epoch 1, Batch 100/2094, Loss: 0.4136, Accuracy: 0.8075\n",
      "Epoch 1, Batch 110/2094, Loss: 0.4112, Accuracy: 0.8114\n",
      "Epoch 1, Batch 120/2094, Loss: 0.3895, Accuracy: 0.8229\n",
      "Epoch 1, Batch 130/2094, Loss: 0.3869, Accuracy: 0.8231\n",
      "Epoch 1, Batch 140/2094, Loss: 0.3757, Accuracy: 0.8304\n",
      "Epoch 1, Batch 150/2094, Loss: 0.3669, Accuracy: 0.8333\n",
      "Epoch 1, Batch 160/2094, Loss: 0.3543, Accuracy: 0.8406\n",
      "Epoch 1, Batch 170/2094, Loss: 0.3447, Accuracy: 0.8456\n",
      "Epoch 1, Batch 180/2094, Loss: 0.3315, Accuracy: 0.8514\n",
      "Epoch 1, Batch 190/2094, Loss: 0.3228, Accuracy: 0.8553\n",
      "Epoch 1, Batch 200/2094, Loss: 0.3099, Accuracy: 0.8625\n",
      "Epoch 1, Batch 210/2094, Loss: 0.2977, Accuracy: 0.8679\n",
      "Epoch 1, Batch 220/2094, Loss: 0.2894, Accuracy: 0.8727\n",
      "Epoch 1, Batch 230/2094, Loss: 0.2784, Accuracy: 0.8783\n",
      "Epoch 1, Batch 240/2094, Loss: 0.2720, Accuracy: 0.8812\n",
      "Epoch 1, Batch 250/2094, Loss: 0.2634, Accuracy: 0.8850\n",
      "Epoch 1, Batch 260/2094, Loss: 0.2617, Accuracy: 0.8856\n",
      "Epoch 1, Batch 270/2094, Loss: 0.2595, Accuracy: 0.8870\n",
      "Epoch 1, Batch 280/2094, Loss: 0.2578, Accuracy: 0.8884\n",
      "Epoch 1, Batch 290/2094, Loss: 0.2546, Accuracy: 0.8897\n",
      "Epoch 1, Batch 300/2094, Loss: 0.2560, Accuracy: 0.8900\n",
      "Epoch 1, Batch 310/2094, Loss: 0.2563, Accuracy: 0.8911\n",
      "Epoch 1, Batch 320/2094, Loss: 0.2552, Accuracy: 0.8930\n",
      "Epoch 1, Batch 330/2094, Loss: 0.2507, Accuracy: 0.8955\n",
      "Epoch 1, Batch 340/2094, Loss: 0.2486, Accuracy: 0.8971\n",
      "Epoch 1, Batch 350/2094, Loss: 0.2466, Accuracy: 0.8979\n",
      "Epoch 1, Batch 360/2094, Loss: 0.2438, Accuracy: 0.9000\n",
      "Epoch 1, Batch 370/2094, Loss: 0.2421, Accuracy: 0.9000\n",
      "Epoch 1, Batch 380/2094, Loss: 0.2401, Accuracy: 0.9013\n",
      "Epoch 1, Batch 390/2094, Loss: 0.2381, Accuracy: 0.9026\n",
      "Epoch 1, Batch 400/2094, Loss: 0.2372, Accuracy: 0.9031\n",
      "Epoch 1, Batch 410/2094, Loss: 0.2340, Accuracy: 0.9049\n",
      "Epoch 1, Batch 420/2094, Loss: 0.2307, Accuracy: 0.9065\n",
      "Epoch 1, Batch 430/2094, Loss: 0.2295, Accuracy: 0.9070\n",
      "Epoch 1, Batch 440/2094, Loss: 0.2251, Accuracy: 0.9091\n",
      "Epoch 1, Batch 450/2094, Loss: 0.2237, Accuracy: 0.9094\n",
      "Epoch 1, Batch 460/2094, Loss: 0.2285, Accuracy: 0.9065\n",
      "Epoch 1, Batch 470/2094, Loss: 0.2278, Accuracy: 0.9080\n",
      "Epoch 1, Batch 480/2094, Loss: 0.2264, Accuracy: 0.9094\n",
      "Epoch 1, Batch 490/2094, Loss: 0.2237, Accuracy: 0.9107\n",
      "Epoch 1, Batch 500/2094, Loss: 0.2204, Accuracy: 0.9120\n",
      "Epoch 1, Batch 510/2094, Loss: 0.2226, Accuracy: 0.9108\n",
      "Epoch 1, Batch 520/2094, Loss: 0.2209, Accuracy: 0.9115\n",
      "Epoch 1, Batch 530/2094, Loss: 0.2208, Accuracy: 0.9118\n",
      "Epoch 1, Batch 540/2094, Loss: 0.2233, Accuracy: 0.9102\n",
      "Epoch 1, Batch 550/2094, Loss: 0.2256, Accuracy: 0.9091\n",
      "Epoch 1, Batch 560/2094, Loss: 0.2237, Accuracy: 0.9098\n",
      "Epoch 1, Batch 570/2094, Loss: 0.2218, Accuracy: 0.9105\n",
      "Epoch 1, Batch 580/2094, Loss: 0.2197, Accuracy: 0.9116\n",
      "Epoch 1, Batch 590/2094, Loss: 0.2164, Accuracy: 0.9131\n",
      "Epoch 1, Batch 600/2094, Loss: 0.2143, Accuracy: 0.9137\n",
      "Epoch 1, Batch 610/2094, Loss: 0.2180, Accuracy: 0.9131\n",
      "Epoch 1, Batch 620/2094, Loss: 0.2176, Accuracy: 0.9133\n",
      "Epoch 1, Batch 630/2094, Loss: 0.2152, Accuracy: 0.9143\n",
      "Epoch 1, Batch 640/2094, Loss: 0.2124, Accuracy: 0.9156\n",
      "Epoch 1, Batch 650/2094, Loss: 0.2094, Accuracy: 0.9169\n",
      "Epoch 1, Batch 660/2094, Loss: 0.2074, Accuracy: 0.9178\n",
      "Epoch 1, Batch 670/2094, Loss: 0.2050, Accuracy: 0.9190\n",
      "Epoch 1, Batch 680/2094, Loss: 0.2026, Accuracy: 0.9202\n",
      "Epoch 1, Batch 690/2094, Loss: 0.1999, Accuracy: 0.9214\n",
      "Epoch 1, Batch 700/2094, Loss: 0.2012, Accuracy: 0.9211\n",
      "Epoch 1, Batch 710/2094, Loss: 0.2000, Accuracy: 0.9215\n",
      "Epoch 1, Batch 720/2094, Loss: 0.1991, Accuracy: 0.9219\n",
      "Epoch 1, Batch 730/2094, Loss: 0.1971, Accuracy: 0.9229\n",
      "Epoch 1, Batch 740/2094, Loss: 0.1973, Accuracy: 0.9233\n",
      "Epoch 1, Batch 750/2094, Loss: 0.1973, Accuracy: 0.9230\n",
      "Epoch 1, Batch 760/2094, Loss: 0.1970, Accuracy: 0.9234\n",
      "Epoch 1, Batch 770/2094, Loss: 0.1967, Accuracy: 0.9231\n",
      "Epoch 1, Batch 780/2094, Loss: 0.1946, Accuracy: 0.9237\n",
      "Epoch 1, Batch 790/2094, Loss: 0.1940, Accuracy: 0.9241\n",
      "Epoch 1, Batch 800/2094, Loss: 0.1929, Accuracy: 0.9244\n",
      "Epoch 1, Batch 810/2094, Loss: 0.1941, Accuracy: 0.9238\n",
      "Epoch 1, Batch 820/2094, Loss: 0.1938, Accuracy: 0.9241\n",
      "Epoch 1, Batch 830/2094, Loss: 0.1928, Accuracy: 0.9247\n",
      "Epoch 1, Batch 840/2094, Loss: 0.1921, Accuracy: 0.9250\n",
      "Epoch 1, Batch 850/2094, Loss: 0.1901, Accuracy: 0.9259\n",
      "Epoch 1, Batch 860/2094, Loss: 0.1888, Accuracy: 0.9265\n",
      "Epoch 1, Batch 870/2094, Loss: 0.1893, Accuracy: 0.9259\n",
      "Epoch 1, Batch 880/2094, Loss: 0.1877, Accuracy: 0.9267\n",
      "Epoch 1, Batch 890/2094, Loss: 0.1872, Accuracy: 0.9270\n",
      "Epoch 1, Batch 900/2094, Loss: 0.1877, Accuracy: 0.9269\n",
      "Epoch 1, Batch 910/2094, Loss: 0.1862, Accuracy: 0.9277\n",
      "Epoch 1, Batch 920/2094, Loss: 0.1854, Accuracy: 0.9277\n",
      "Epoch 1, Batch 930/2094, Loss: 0.1858, Accuracy: 0.9280\n",
      "Epoch 1, Batch 940/2094, Loss: 0.1855, Accuracy: 0.9282\n",
      "Epoch 1, Batch 950/2094, Loss: 0.1850, Accuracy: 0.9284\n",
      "Epoch 1, Batch 960/2094, Loss: 0.1847, Accuracy: 0.9284\n",
      "Epoch 1, Batch 970/2094, Loss: 0.1835, Accuracy: 0.9289\n",
      "Epoch 1, Batch 980/2094, Loss: 0.1831, Accuracy: 0.9288\n",
      "Epoch 1, Batch 990/2094, Loss: 0.1834, Accuracy: 0.9290\n",
      "Epoch 1, Batch 1000/2094, Loss: 0.1822, Accuracy: 0.9295\n",
      "Epoch 1, Batch 1010/2094, Loss: 0.1807, Accuracy: 0.9302\n",
      "Epoch 1, Batch 1020/2094, Loss: 0.1791, Accuracy: 0.9309\n",
      "Epoch 1, Batch 1030/2094, Loss: 0.1778, Accuracy: 0.9316\n",
      "Epoch 1, Batch 1040/2094, Loss: 0.1773, Accuracy: 0.9315\n",
      "Epoch 1, Batch 1050/2094, Loss: 0.1757, Accuracy: 0.9321\n",
      "Epoch 1, Batch 1060/2094, Loss: 0.1749, Accuracy: 0.9323\n",
      "Epoch 1, Batch 1070/2094, Loss: 0.1746, Accuracy: 0.9325\n",
      "Epoch 1, Batch 1080/2094, Loss: 0.1734, Accuracy: 0.9329\n",
      "Epoch 1, Batch 1090/2094, Loss: 0.1732, Accuracy: 0.9330\n",
      "Epoch 1, Batch 1100/2094, Loss: 0.1731, Accuracy: 0.9327\n",
      "Epoch 1, Batch 1110/2094, Loss: 0.1722, Accuracy: 0.9331\n",
      "Epoch 1, Batch 1120/2094, Loss: 0.1714, Accuracy: 0.9333\n",
      "Epoch 1, Batch 1130/2094, Loss: 0.1707, Accuracy: 0.9336\n",
      "Epoch 1, Batch 1140/2094, Loss: 0.1696, Accuracy: 0.9342\n",
      "Epoch 1, Batch 1150/2094, Loss: 0.1683, Accuracy: 0.9348\n",
      "Epoch 1, Batch 1160/2094, Loss: 0.1669, Accuracy: 0.9353\n",
      "Epoch 1, Batch 1170/2094, Loss: 0.1656, Accuracy: 0.9359\n",
      "Epoch 1, Batch 1180/2094, Loss: 0.1655, Accuracy: 0.9362\n",
      "Epoch 1, Batch 1190/2094, Loss: 0.1663, Accuracy: 0.9361\n",
      "Epoch 1, Batch 1200/2094, Loss: 0.1651, Accuracy: 0.9367\n",
      "Epoch 1, Batch 1210/2094, Loss: 0.1651, Accuracy: 0.9368\n",
      "Epoch 1, Batch 1220/2094, Loss: 0.1645, Accuracy: 0.9371\n",
      "Epoch 1, Batch 1230/2094, Loss: 0.1636, Accuracy: 0.9376\n",
      "Epoch 1, Batch 1240/2094, Loss: 0.1626, Accuracy: 0.9381\n",
      "Epoch 1, Batch 1250/2094, Loss: 0.1617, Accuracy: 0.9384\n",
      "Epoch 1, Batch 1260/2094, Loss: 0.1605, Accuracy: 0.9389\n",
      "Epoch 1, Batch 1270/2094, Loss: 0.1593, Accuracy: 0.9394\n",
      "Epoch 1, Batch 1280/2094, Loss: 0.1600, Accuracy: 0.9393\n",
      "Epoch 1, Batch 1290/2094, Loss: 0.1598, Accuracy: 0.9393\n",
      "Epoch 1, Batch 1300/2094, Loss: 0.1588, Accuracy: 0.9398\n",
      "Epoch 1, Batch 1310/2094, Loss: 0.1579, Accuracy: 0.9403\n",
      "Epoch 1, Batch 1320/2094, Loss: 0.1573, Accuracy: 0.9405\n",
      "Epoch 1, Batch 1330/2094, Loss: 0.1566, Accuracy: 0.9408\n",
      "Epoch 1, Batch 1340/2094, Loss: 0.1563, Accuracy: 0.9409\n",
      "Epoch 1, Batch 1350/2094, Loss: 0.1557, Accuracy: 0.9411\n",
      "Epoch 1, Batch 1360/2094, Loss: 0.1548, Accuracy: 0.9415\n",
      "Epoch 1, Batch 1370/2094, Loss: 0.1537, Accuracy: 0.9420\n",
      "Epoch 1, Batch 1380/2094, Loss: 0.1539, Accuracy: 0.9420\n",
      "Epoch 1, Batch 1390/2094, Loss: 0.1531, Accuracy: 0.9423\n",
      "Epoch 1, Batch 1400/2094, Loss: 0.1520, Accuracy: 0.9427\n",
      "Epoch 1, Batch 1410/2094, Loss: 0.1517, Accuracy: 0.9429\n",
      "Epoch 1, Batch 1420/2094, Loss: 0.1515, Accuracy: 0.9431\n",
      "Epoch 1, Batch 1430/2094, Loss: 0.1505, Accuracy: 0.9435\n",
      "Epoch 1, Batch 1440/2094, Loss: 0.1496, Accuracy: 0.9439\n",
      "Epoch 1, Batch 1450/2094, Loss: 0.1487, Accuracy: 0.9443\n",
      "Epoch 1, Batch 1460/2094, Loss: 0.1492, Accuracy: 0.9443\n",
      "Epoch 1, Batch 1470/2094, Loss: 0.1484, Accuracy: 0.9446\n",
      "Epoch 1, Batch 1480/2094, Loss: 0.1485, Accuracy: 0.9448\n",
      "Epoch 1, Batch 1490/2094, Loss: 0.1489, Accuracy: 0.9448\n",
      "Epoch 1, Batch 1500/2094, Loss: 0.1482, Accuracy: 0.9450\n",
      "Epoch 1, Batch 1510/2094, Loss: 0.1477, Accuracy: 0.9452\n",
      "Epoch 1, Batch 1520/2094, Loss: 0.1473, Accuracy: 0.9454\n",
      "Epoch 1, Batch 1530/2094, Loss: 0.1469, Accuracy: 0.9456\n",
      "Epoch 1, Batch 1540/2094, Loss: 0.1473, Accuracy: 0.9456\n",
      "Epoch 1, Batch 1550/2094, Loss: 0.1465, Accuracy: 0.9460\n",
      "Epoch 1, Batch 1560/2094, Loss: 0.1464, Accuracy: 0.9458\n",
      "Epoch 1, Batch 1570/2094, Loss: 0.1470, Accuracy: 0.9457\n",
      "Epoch 1, Batch 1580/2094, Loss: 0.1466, Accuracy: 0.9457\n",
      "Epoch 1, Batch 1590/2094, Loss: 0.1465, Accuracy: 0.9459\n",
      "Epoch 1, Batch 1600/2094, Loss: 0.1460, Accuracy: 0.9459\n",
      "Epoch 1, Batch 1610/2094, Loss: 0.1457, Accuracy: 0.9460\n",
      "Epoch 1, Batch 1620/2094, Loss: 0.1454, Accuracy: 0.9461\n",
      "Epoch 1, Batch 1630/2094, Loss: 0.1452, Accuracy: 0.9462\n",
      "Epoch 1, Batch 1640/2094, Loss: 0.1445, Accuracy: 0.9465\n",
      "Epoch 1, Batch 1650/2094, Loss: 0.1449, Accuracy: 0.9465\n",
      "Epoch 1, Batch 1660/2094, Loss: 0.1449, Accuracy: 0.9464\n",
      "Epoch 1, Batch 1670/2094, Loss: 0.1452, Accuracy: 0.9463\n",
      "Epoch 1, Batch 1680/2094, Loss: 0.1451, Accuracy: 0.9463\n",
      "Epoch 1, Batch 1690/2094, Loss: 0.1455, Accuracy: 0.9462\n",
      "Epoch 1, Batch 1700/2094, Loss: 0.1451, Accuracy: 0.9463\n",
      "Epoch 1, Batch 1710/2094, Loss: 0.1454, Accuracy: 0.9462\n",
      "Epoch 1, Batch 1720/2094, Loss: 0.1457, Accuracy: 0.9458\n",
      "Epoch 1, Batch 1730/2094, Loss: 0.1455, Accuracy: 0.9460\n",
      "Epoch 1, Batch 1740/2094, Loss: 0.1450, Accuracy: 0.9461\n",
      "Epoch 1, Batch 1750/2094, Loss: 0.1448, Accuracy: 0.9463\n",
      "Epoch 1, Batch 1760/2094, Loss: 0.1440, Accuracy: 0.9466\n",
      "Epoch 1, Batch 1770/2094, Loss: 0.1433, Accuracy: 0.9469\n",
      "Epoch 1, Batch 1780/2094, Loss: 0.1440, Accuracy: 0.9468\n",
      "Epoch 1, Batch 1790/2094, Loss: 0.1436, Accuracy: 0.9469\n",
      "Epoch 1, Batch 1800/2094, Loss: 0.1431, Accuracy: 0.9471\n",
      "Epoch 1, Batch 1810/2094, Loss: 0.1428, Accuracy: 0.9471\n",
      "Epoch 1, Batch 1820/2094, Loss: 0.1432, Accuracy: 0.9470\n",
      "Epoch 1, Batch 1830/2094, Loss: 0.1431, Accuracy: 0.9470\n",
      "Epoch 1, Batch 1840/2094, Loss: 0.1429, Accuracy: 0.9470\n",
      "Epoch 1, Batch 1850/2094, Loss: 0.1429, Accuracy: 0.9468\n",
      "Epoch 1, Batch 1860/2094, Loss: 0.1422, Accuracy: 0.9470\n",
      "Epoch 1, Batch 1870/2094, Loss: 0.1417, Accuracy: 0.9472\n",
      "Epoch 1, Batch 1880/2094, Loss: 0.1409, Accuracy: 0.9475\n",
      "Epoch 1, Batch 1890/2094, Loss: 0.1403, Accuracy: 0.9478\n",
      "Epoch 1, Batch 1900/2094, Loss: 0.1409, Accuracy: 0.9478\n",
      "Epoch 1, Batch 1910/2094, Loss: 0.1409, Accuracy: 0.9478\n",
      "Epoch 1, Batch 1920/2094, Loss: 0.1410, Accuracy: 0.9478\n",
      "Epoch 1, Batch 1930/2094, Loss: 0.1407, Accuracy: 0.9479\n",
      "Epoch 1, Batch 1940/2094, Loss: 0.1404, Accuracy: 0.9481\n",
      "Epoch 1, Batch 1950/2094, Loss: 0.1397, Accuracy: 0.9483\n",
      "Epoch 1, Batch 1960/2094, Loss: 0.1392, Accuracy: 0.9485\n",
      "Epoch 1, Batch 1970/2094, Loss: 0.1394, Accuracy: 0.9485\n",
      "Epoch 1, Batch 1980/2094, Loss: 0.1391, Accuracy: 0.9486\n",
      "Epoch 1, Batch 1990/2094, Loss: 0.1385, Accuracy: 0.9489\n",
      "Epoch 1, Batch 2000/2094, Loss: 0.1387, Accuracy: 0.9489\n",
      "Epoch 1, Batch 2010/2094, Loss: 0.1388, Accuracy: 0.9490\n",
      "Epoch 1, Batch 2020/2094, Loss: 0.1384, Accuracy: 0.9491\n",
      "Epoch 1, Batch 2030/2094, Loss: 0.1378, Accuracy: 0.9494\n",
      "Epoch 1, Batch 2040/2094, Loss: 0.1383, Accuracy: 0.9494\n",
      "Epoch 1, Batch 2050/2094, Loss: 0.1384, Accuracy: 0.9494\n",
      "Epoch 1, Batch 2060/2094, Loss: 0.1386, Accuracy: 0.9494\n",
      "Epoch 1, Batch 2070/2094, Loss: 0.1380, Accuracy: 0.9496\n",
      "Epoch 1, Batch 2080/2094, Loss: 0.1375, Accuracy: 0.9499\n",
      "Epoch 1, Batch 2090/2094, Loss: 0.1376, Accuracy: 0.9500\n",
      "Epoch 1, Loss: 0.1377, Accuracy: 0.9500\n",
      "Test Accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели на GPU\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    albert_model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch_num, batch in enumerate(train_loader, 1):\n",
    "        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  \n",
    "            outputs = albert_model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == batch[2].to(device)).item()\n",
    "\n",
    "        if batch_num % 10 == 0:\n",
    "            avg_loss = total_loss / batch_num\n",
    "            accuracy = correct_predictions / (batch_num * train_loader.batch_size)\n",
    "            print(f'Epoch {epoch + 1}, Batch {batch_num}/{len(train_loader)}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / (len(train_loader) * train_loader.batch_size)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    albert_model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_loader:\n",
    "            test_inputs = {'input_ids': test_batch[0].to(device), 'attention_mask': test_batch[1].to(device)}\n",
    "            test_outputs = albert_model(**test_inputs)\n",
    "            test_logits = test_outputs.logits\n",
    "            test_preds = torch.argmax(test_logits, dim=1)\n",
    "            all_preds.extend(test_preds.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_labels.cpu().numpy(), all_preds)\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "albert_model.save_pretrained('albert_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
