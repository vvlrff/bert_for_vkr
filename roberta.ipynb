{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Проверяем доступность GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных из CSV-файла\n",
    "df = pd.read_csv('your_dataset.csv')  # Замените 'your_dataset.csv' на путь к вашему файлу CSV\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Подготовка текстов и меток\n",
    "train_texts, train_labels = train_df['news_headline'].tolist(), train_df['category_num'].tolist()\n",
    "test_texts, test_labels = test_df['news_headline'].tolist(), test_df['category_num'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\vvlrff\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Инициализация токенизатора и преобразование текста в токены\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Преобразование меток в тензоры\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Перемещение данных на GPU\n",
    "train_encodings = {key: value.to(device) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: value.to(device) for key, value in test_encodings.items()}\n",
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Создание DataLoader для обучения и тестирования\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Инициализация модели RoBERTa для классификации на GPU\n",
    "num_classes = 2  # Замените 2 на количество классов в вашем наборе данных\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Инициализация оптимизатора\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/1047, Loss: 0.6988, Accuracy: 0.5250\n",
      "Epoch 1, Batch 20/1047, Loss: 0.5987, Accuracy: 0.6438\n",
      "Epoch 1, Batch 30/1047, Loss: 0.5133, Accuracy: 0.7208\n",
      "Epoch 1, Batch 40/1047, Loss: 0.4432, Accuracy: 0.7750\n",
      "Epoch 1, Batch 50/1047, Loss: 0.4178, Accuracy: 0.8025\n",
      "Epoch 1, Batch 60/1047, Loss: 0.4009, Accuracy: 0.8187\n",
      "Epoch 1, Batch 70/1047, Loss: 0.3851, Accuracy: 0.8321\n",
      "Epoch 1, Batch 80/1047, Loss: 0.3717, Accuracy: 0.8422\n",
      "Epoch 1, Batch 90/1047, Loss: 0.3475, Accuracy: 0.8542\n",
      "Epoch 1, Batch 100/1047, Loss: 0.3228, Accuracy: 0.8662\n",
      "Epoch 1, Batch 110/1047, Loss: 0.2968, Accuracy: 0.8784\n",
      "Epoch 1, Batch 120/1047, Loss: 0.2825, Accuracy: 0.8854\n",
      "Epoch 1, Batch 130/1047, Loss: 0.2684, Accuracy: 0.8913\n",
      "Epoch 1, Batch 140/1047, Loss: 0.2519, Accuracy: 0.8982\n",
      "Epoch 1, Batch 150/1047, Loss: 0.2490, Accuracy: 0.9000\n",
      "Epoch 1, Batch 160/1047, Loss: 0.2405, Accuracy: 0.9039\n",
      "Epoch 1, Batch 170/1047, Loss: 0.2385, Accuracy: 0.9066\n",
      "Epoch 1, Batch 180/1047, Loss: 0.2338, Accuracy: 0.9090\n",
      "Epoch 1, Batch 190/1047, Loss: 0.2261, Accuracy: 0.9132\n",
      "Epoch 1, Batch 200/1047, Loss: 0.2214, Accuracy: 0.9156\n",
      "Epoch 1, Batch 210/1047, Loss: 0.2160, Accuracy: 0.9167\n",
      "Epoch 1, Batch 220/1047, Loss: 0.2125, Accuracy: 0.9170\n",
      "Epoch 1, Batch 230/1047, Loss: 0.2049, Accuracy: 0.9201\n",
      "Epoch 1, Batch 240/1047, Loss: 0.2003, Accuracy: 0.9224\n",
      "Epoch 1, Batch 250/1047, Loss: 0.2000, Accuracy: 0.9230\n",
      "Epoch 1, Batch 260/1047, Loss: 0.1973, Accuracy: 0.9240\n",
      "Epoch 1, Batch 270/1047, Loss: 0.1936, Accuracy: 0.9259\n",
      "Epoch 1, Batch 280/1047, Loss: 0.1899, Accuracy: 0.9277\n",
      "Epoch 1, Batch 290/1047, Loss: 0.1849, Accuracy: 0.9293\n",
      "Epoch 1, Batch 300/1047, Loss: 0.1823, Accuracy: 0.9304\n",
      "Epoch 1, Batch 310/1047, Loss: 0.1800, Accuracy: 0.9315\n",
      "Epoch 1, Batch 320/1047, Loss: 0.1800, Accuracy: 0.9316\n",
      "Epoch 1, Batch 330/1047, Loss: 0.1771, Accuracy: 0.9330\n",
      "Epoch 1, Batch 340/1047, Loss: 0.1734, Accuracy: 0.9342\n",
      "Epoch 1, Batch 350/1047, Loss: 0.1755, Accuracy: 0.9336\n",
      "Epoch 1, Batch 360/1047, Loss: 0.1723, Accuracy: 0.9351\n",
      "Epoch 1, Batch 370/1047, Loss: 0.1695, Accuracy: 0.9365\n",
      "Epoch 1, Batch 380/1047, Loss: 0.1685, Accuracy: 0.9368\n",
      "Epoch 1, Batch 390/1047, Loss: 0.1654, Accuracy: 0.9381\n",
      "Epoch 1, Batch 400/1047, Loss: 0.1654, Accuracy: 0.9387\n",
      "Epoch 1, Batch 410/1047, Loss: 0.1660, Accuracy: 0.9381\n",
      "Epoch 1, Batch 420/1047, Loss: 0.1647, Accuracy: 0.9387\n",
      "Epoch 1, Batch 430/1047, Loss: 0.1615, Accuracy: 0.9398\n",
      "Epoch 1, Batch 440/1047, Loss: 0.1608, Accuracy: 0.9406\n",
      "Epoch 1, Batch 450/1047, Loss: 0.1609, Accuracy: 0.9403\n",
      "Epoch 1, Batch 460/1047, Loss: 0.1612, Accuracy: 0.9399\n",
      "Epoch 1, Batch 470/1047, Loss: 0.1587, Accuracy: 0.9412\n",
      "Epoch 1, Batch 480/1047, Loss: 0.1564, Accuracy: 0.9422\n",
      "Epoch 1, Batch 490/1047, Loss: 0.1540, Accuracy: 0.9429\n",
      "Epoch 1, Batch 500/1047, Loss: 0.1528, Accuracy: 0.9435\n",
      "Epoch 1, Batch 510/1047, Loss: 0.1505, Accuracy: 0.9444\n",
      "Epoch 1, Batch 520/1047, Loss: 0.1490, Accuracy: 0.9447\n",
      "Epoch 1, Batch 530/1047, Loss: 0.1482, Accuracy: 0.9455\n",
      "Epoch 1, Batch 540/1047, Loss: 0.1471, Accuracy: 0.9461\n",
      "Epoch 1, Batch 550/1047, Loss: 0.1457, Accuracy: 0.9464\n",
      "Epoch 1, Batch 560/1047, Loss: 0.1463, Accuracy: 0.9469\n",
      "Epoch 1, Batch 570/1047, Loss: 0.1457, Accuracy: 0.9467\n",
      "Epoch 1, Batch 580/1047, Loss: 0.1444, Accuracy: 0.9472\n",
      "Epoch 1, Batch 590/1047, Loss: 0.1425, Accuracy: 0.9479\n",
      "Epoch 1, Batch 600/1047, Loss: 0.1412, Accuracy: 0.9483\n",
      "Epoch 1, Batch 610/1047, Loss: 0.1411, Accuracy: 0.9480\n",
      "Epoch 1, Batch 620/1047, Loss: 0.1406, Accuracy: 0.9482\n",
      "Epoch 1, Batch 630/1047, Loss: 0.1401, Accuracy: 0.9486\n",
      "Epoch 1, Batch 640/1047, Loss: 0.1393, Accuracy: 0.9490\n",
      "Epoch 1, Batch 650/1047, Loss: 0.1395, Accuracy: 0.9485\n",
      "Epoch 1, Batch 660/1047, Loss: 0.1379, Accuracy: 0.9491\n",
      "Epoch 1, Batch 670/1047, Loss: 0.1364, Accuracy: 0.9494\n",
      "Epoch 1, Batch 680/1047, Loss: 0.1375, Accuracy: 0.9493\n",
      "Epoch 1, Batch 690/1047, Loss: 0.1371, Accuracy: 0.9496\n",
      "Epoch 1, Batch 700/1047, Loss: 0.1356, Accuracy: 0.9502\n",
      "Epoch 1, Batch 710/1047, Loss: 0.1346, Accuracy: 0.9507\n",
      "Epoch 1, Batch 720/1047, Loss: 0.1338, Accuracy: 0.9510\n",
      "Epoch 1, Batch 730/1047, Loss: 0.1329, Accuracy: 0.9515\n",
      "Epoch 1, Batch 740/1047, Loss: 0.1321, Accuracy: 0.9517\n",
      "Epoch 1, Batch 750/1047, Loss: 0.1309, Accuracy: 0.9522\n",
      "Epoch 1, Batch 760/1047, Loss: 0.1299, Accuracy: 0.9525\n",
      "Epoch 1, Batch 770/1047, Loss: 0.1284, Accuracy: 0.9529\n",
      "Epoch 1, Batch 780/1047, Loss: 0.1277, Accuracy: 0.9534\n",
      "Epoch 1, Batch 790/1047, Loss: 0.1264, Accuracy: 0.9538\n",
      "Epoch 1, Batch 800/1047, Loss: 0.1249, Accuracy: 0.9544\n",
      "Epoch 1, Batch 810/1047, Loss: 0.1258, Accuracy: 0.9542\n",
      "Epoch 1, Batch 820/1047, Loss: 0.1253, Accuracy: 0.9544\n",
      "Epoch 1, Batch 830/1047, Loss: 0.1243, Accuracy: 0.9548\n",
      "Epoch 1, Batch 840/1047, Loss: 0.1236, Accuracy: 0.9551\n",
      "Epoch 1, Batch 850/1047, Loss: 0.1233, Accuracy: 0.9551\n",
      "Epoch 1, Batch 860/1047, Loss: 0.1224, Accuracy: 0.9554\n",
      "Epoch 1, Batch 870/1047, Loss: 0.1218, Accuracy: 0.9556\n",
      "Epoch 1, Batch 880/1047, Loss: 0.1205, Accuracy: 0.9561\n",
      "Epoch 1, Batch 890/1047, Loss: 0.1199, Accuracy: 0.9563\n",
      "Epoch 1, Batch 900/1047, Loss: 0.1193, Accuracy: 0.9564\n",
      "Epoch 1, Batch 910/1047, Loss: 0.1191, Accuracy: 0.9563\n",
      "Epoch 1, Batch 920/1047, Loss: 0.1191, Accuracy: 0.9567\n",
      "Epoch 1, Batch 930/1047, Loss: 0.1179, Accuracy: 0.9571\n",
      "Epoch 1, Batch 940/1047, Loss: 0.1169, Accuracy: 0.9574\n",
      "Epoch 1, Batch 950/1047, Loss: 0.1157, Accuracy: 0.9579\n",
      "Epoch 1, Batch 960/1047, Loss: 0.1159, Accuracy: 0.9579\n",
      "Epoch 1, Batch 970/1047, Loss: 0.1155, Accuracy: 0.9581\n",
      "Epoch 1, Batch 980/1047, Loss: 0.1167, Accuracy: 0.9582\n",
      "Epoch 1, Batch 990/1047, Loss: 0.1167, Accuracy: 0.9581\n",
      "Epoch 1, Batch 1000/1047, Loss: 0.1170, Accuracy: 0.9580\n",
      "Epoch 1, Batch 1010/1047, Loss: 0.1163, Accuracy: 0.9583\n",
      "Epoch 1, Batch 1020/1047, Loss: 0.1155, Accuracy: 0.9585\n",
      "Epoch 1, Batch 1030/1047, Loss: 0.1150, Accuracy: 0.9586\n",
      "Epoch 1, Batch 1040/1047, Loss: 0.1141, Accuracy: 0.9590\n",
      "Epoch 1, Loss: 0.1144, Accuracy: 0.9590\n",
      "Test Accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели на GPU\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_num, batch in enumerate(train_loader, 1):\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}  # Перемещаем данные на GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Считаем общую потерю\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Считаем количество правильных предсказаний\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == batch[2]).item()\n",
    "\n",
    "        # Выводим лог каждые, например, 10 батчей\n",
    "        if batch_num % 10 == 0:\n",
    "            avg_loss = total_loss / batch_num\n",
    "            accuracy = correct_predictions / (batch_num * train_loader.batch_size)\n",
    "            print(f'Epoch {epoch + 1}, Batch {batch_num}/{len(train_loader)}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Выводим лог в конце каждой эпохи\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / (len(train_loader) * train_loader.batch_size)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Проверка точности на тестовом наборе\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_loader:\n",
    "            test_inputs = {'input_ids': test_batch[0].to(device), 'attention_mask': test_batch[1].to(device)}\n",
    "            test_outputs = model(**test_inputs)\n",
    "            test_logits = test_outputs.logits\n",
    "            test_preds = torch.argmax(test_logits, dim=1)\n",
    "            all_preds.extend(test_preds.cpu().numpy())\n",
    "\n",
    "    # Вычисление и вывод точности на тестовом наборе\n",
    "    test_accuracy = accuracy_score(test_labels.cpu().numpy(), all_preds)\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "model.save_pretrained('roberta_classification_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
